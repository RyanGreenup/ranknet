{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Neural Networks\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "[Tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "source": [
    "## Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Load the Data using pandas/numpy:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./DataSets/winequality-red.csv', sep=';')\n",
    "df=np.array(df.values)"
   ]
  },
  {
   "source": [
    "The last column are the features so extract those:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[:,-1]\n",
    "X = df[:,range(df.shape[1]-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y>5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_good = y\n",
    "prob_bad = [ not i for i in y ]\n",
    "prob_good_bad = np.c_[prob_good, prob_bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[False  True]\n [False  True]\n [False  True]\n ...\n [ True False]\n [False  True]\n [ True False]]\n"
     ]
    }
   ],
   "source": [
    "print(prob_good_bad)"
   ]
  },
  {
   "source": [
    "Finally make sure that these are tensors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0., 1.],\n        [0., 1.],\n        [0., 1.],\n        ...,\n        [1., 0.],\n        [0., 1.],\n        [1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.from_numpy(X.astype(np.float32))\n",
    "prob_good_bad = torch.from_numpy(prob_good_bad.astype(np.float32))\n",
    "y = prob_good_bad\n",
    "print(y)\n"
   ]
  },
  {
   "source": [
    "## Define a Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=11, out_features=4),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(4, 2),\n",
    "    nn.Sigmoid()\n",
    "    # nn.Flatten(start_dim=0, end_dim=1)\n",
    ")"
   ]
  },
  {
   "source": [
    "### Define a Loss Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "lss_fn = torch.nn.NLLLoss()\n"
   ]
  },
  {
   "source": [
    "### Define an Optimizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(), lr = 1e-6)"
   ]
  },
  {
   "source": [
    "### Train the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "99 \t 865.0260009765625\n",
      "199 \t 864.8231811523438\n",
      "299 \t 864.6378173828125\n",
      "399 \t 864.4583129882812\n",
      "499 \t 864.280517578125\n",
      "599 \t 864.1033935546875\n",
      "699 \t 863.9268188476562\n",
      "799 \t 863.750244140625\n",
      "899 \t 863.5740966796875\n",
      "999 \t 863.3980102539062\n",
      "1099 \t 863.2222290039062\n",
      "1199 \t 863.046630859375\n",
      "1299 \t 862.8712768554688\n",
      "1399 \t 862.6961059570312\n",
      "1499 \t 862.521240234375\n",
      "1599 \t 862.3466796875\n",
      "1699 \t 862.1724853515625\n",
      "1799 \t 861.99853515625\n",
      "1899 \t 861.82470703125\n",
      "1999 \t 861.6512451171875\n",
      "2099 \t 861.4779663085938\n",
      "2199 \t 861.3048095703125\n",
      "2299 \t 861.1320190429688\n",
      "2399 \t 860.9593505859375\n",
      "2499 \t 860.7869873046875\n",
      "2599 \t 860.6148071289062\n",
      "2699 \t 860.4428100585938\n",
      "2799 \t 860.2710571289062\n",
      "2899 \t 860.099609375\n",
      "2999 \t 859.92822265625\n",
      "3099 \t 859.7572021484375\n",
      "3199 \t 859.5863647460938\n",
      "3299 \t 859.415771484375\n",
      "3399 \t 859.245361328125\n",
      "3499 \t 859.0751953125\n",
      "3599 \t 858.9052734375\n",
      "3699 \t 858.735595703125\n",
      "3799 \t 858.5660400390625\n",
      "3899 \t 858.3968505859375\n",
      "3999 \t 858.227783203125\n",
      "4099 \t 858.0590209960938\n",
      "4199 \t 857.8905029296875\n",
      "4299 \t 857.7221069335938\n",
      "4399 \t 857.553955078125\n",
      "4499 \t 857.3861083984375\n",
      "4599 \t 857.2184448242188\n",
      "4699 \t 857.0509643554688\n",
      "4799 \t 856.8837890625\n",
      "4899 \t 856.716796875\n",
      "4999 \t 856.5499267578125\n"
     ]
    }
   ],
   "source": [
    "for t in range(5000):\n",
    "    # Forward Pass: Compute predicted y value\n",
    "    y_pred = model(X.float())\n",
    "\n",
    "    # Measure the Loss\n",
    "    loss = loss_fn(y_pred, y.float()) \n",
    "    if t % 100 == 99:\n",
    "        print(t, '\\t', loss.item())\n",
    "\n",
    "    # Backward Pass; Compute the Partial Derivatives\n",
    "    ## First Zero the Gradients, otherwise the can't be overwritten\n",
    "    optimizer.zero_grad()\n",
    "    ## Now calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Adjust the Weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.4140, 0.6279],\n        [0.4074, 0.6440],\n        [0.4089, 0.6409],\n        ...,\n        [0.3900, 0.6461],\n        [0.3939, 0.6457],\n        [0.4057, 0.6430]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "yhat = model(X)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "descriptive statistics of yhat-------\n0.51529443\n0.10740744\ndescriptive statistics of y-------\ntensor(0.5000)\ntensor(0.5001)\n"
     ]
    }
   ],
   "source": [
    "yhat = model(X)\n",
    "print(\"descriptive statistics of yhat-------\")\n",
    "print(yhat.detach().numpy().mean())\n",
    "print(yhat.detach().numpy().std())\n",
    "\n",
    "print(\"descriptive statistics of y-------\")\n",
    "print(y.mean())\n",
    "print(y.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.41402104 0.62786376]\n [0.40740186 0.6439618 ]\n [0.40887716 0.64087504]\n ...\n [0.39003873 0.6461355 ]\n [0.39392456 0.6457051 ]\n [0.405732   0.6430231 ]]\n"
     ]
    }
   ],
   "source": [
    "print(yhat.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_good_obs = y.detach().numpy()[:,0] > y.detach().numpy()[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_good_pred = yhat.detach().numpy()[:,0] > yhat.detach().numpy()[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4652908067542214"
      ]
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "np.average([ a == b for a in is_good_obs for b in is_good_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " ...]"
      ]
     },
     "metadata": {},
     "execution_count": 135
    }
   ],
   "source": [
    "[ (a, b) for a in y.detach().numpy()[:,0] for b in y.detach().numpy()[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "yhat = (yhat > 0.5).detach().numpy()\n",
    "print(np.average(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0., 0., 0.,  ..., 1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "correctQ = [ i==j for i in yhat for j in y ]\n",
    "print(np.average(correctQ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "print(correctQ)"
   ]
  },
  {
   "source": [
    "## Torch Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Define a torch class, in this case we'll use an 11-4-1, where the output measures whether or not the wine is good."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Network(\n  (hidden1): Linear(in_features=11, out_features=10, bias=True)\n  (hidden2): Linear(in_features=10, out_features=8, bias=True)\n  (hidden3): Linear(in_features=8, out_features=4, bias=True)\n  (output): Linear(in_features=4, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n  (softmax): Softmax(dim=1)\n)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        # Create the input layer and the hidden layer\n",
    "        self.hidden1 = nn.Linear(11, 10)\n",
    "        self.hidden2 = nn.Linear(10, 8)\n",
    "        self.hidden3 = nn.Linear(8, 4)\n",
    "        self.output = nn.Linear(4, 1)\n",
    "\n",
    "        # Define the activation functions that will be used\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1) # dim=1 calculates softmax across cols\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Take input\n",
    "        x = self.hidden1(x)  # Linear Combination of input-> hidden\n",
    "        x = self.hidden2(x)  # Linear Combination of input-> hidden\n",
    "        x = self.hidden3(x)  # Linear Combination of input-> hidden\n",
    "        x = self.sigmoid(x) # Activation Function\n",
    "        x = self.output(x)  # Linear Combination of hidden -> output\n",
    "        x = self.sigmoid(x) # Activation Function\n",
    "\n",
    "        return x\n",
    "\n",
    "# Assign the model object\n",
    "net = Network()\n",
    "print(net)"
   ]
  },
  {
   "source": [
    "### Generate the Model Output"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.2479],\n        [0.2181],\n        [0.2249],\n        ...,\n        [0.2295],\n        [0.2258],\n        [0.2312]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Print the Model Output\n",
    "out = net(X)\n",
    "print(out)"
   ]
  },
  {
   "source": [
    "## Measure the Loss Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First define a loss function,  We'll use *MSE* here, but there are [many others](https://pytorch.org/docs/stable/nn.html#loss-functions). Also define the form of Gradient Descent implemented"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-6\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=1e-3)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=eta, momentum = 0.9)"
   ]
  },
  {
   "source": [
    "## Train the Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now that gradient descent model, the structure of the network and the loss function are all defined we can begin training the network."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "840620.4375\n",
      "636174.0\n",
      "636137.125\n",
      "636131.75\n",
      "636127.4375\n",
      "636124.375\n",
      "636122.5\n",
      "636121.3125\n",
      "636120.75\n",
      "636120.375\n"
     ]
    }
   ],
   "source": [
    "for t in range(1000):\n",
    "    # Forward Pass; Calculate y_pred\n",
    "    y_pred = net(X)\n",
    "\n",
    "    # Calculate and print the loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 0:\n",
    "        print(loss.item())\n",
    "    \n",
    "    # Backward Pass\n",
    "    optimizer.zero_grad()      # Zero out the Gradients\n",
    "    loss.backward()            # Calculate Gradients and store in .grad\n",
    "\n",
    "    # Update the Gradients\n",
    "    optimizer.step()"
   ]
  },
  {
   "source": [
    "### Save the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './wine_neural_network.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "source": [
    "## Print the Parameters of the Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hidden1.weight tensor([[ 0.1659,  0.1932,  0.2630,  0.1430,  0.1354, -0.1183, -0.1731, -0.2000,\n          0.2040, -0.2001,  0.0873],\n        [ 0.2676,  0.1665,  0.0743, -0.0893, -0.0434,  0.1390,  0.0346,  0.0886,\n         -0.0303, -0.2046,  0.1101],\n        [ 0.3145,  0.1626,  0.3498,  0.1678, -0.0362,  0.3579,  0.0022, -0.1147,\n          0.2304,  0.0761,  0.2774],\n        [-0.3507, -0.4197, -0.0269,  0.0961,  0.1233, -0.2793,  0.0695, -0.4159,\n         -0.2709, -0.2752, -0.4242],\n        [ 0.2044,  0.3283,  0.3445, -0.1539,  0.2026,  0.2616,  0.0382,  0.3558,\n         -0.1172,  0.1999,  0.1058],\n        [-0.1351, -0.2686, -0.1736, -0.0905, -0.2516,  0.2392,  0.1405,  0.2328,\n          0.0160, -0.0375, -0.0537],\n        [ 0.3497,  0.2349, -0.0523,  0.2163, -0.1075, -0.1066,  0.3245,  0.0276,\n          0.0748,  0.0826,  0.1978],\n        [ 0.1737, -0.2390, -0.1542,  0.2967, -0.2272, -0.2125, -0.2348,  0.2773,\n          0.2997, -0.0662, -0.0178],\n        [ 0.0682, -0.0585,  0.0850,  0.1409,  0.2425, -0.0265, -0.1851,  0.0038,\n          0.2583,  0.2076,  0.0763],\n        [-0.2085,  0.3028,  0.2479,  0.0274,  0.1494, -0.1037,  0.0908,  0.2072,\n         -0.0195, -0.0609, -0.2057]])\nhidden1.bias tensor([ 0.2069, -0.1582,  0.3704, -0.2281, -0.1732,  0.0478, -0.1810,  0.0602,\n         0.2513,  0.1525])\nhidden2.weight tensor([[-0.0291,  0.1269, -0.3671,  0.1245, -0.2504, -0.1707, -0.2845,  0.0256,\n          0.0964, -0.0428],\n        [ 0.2475,  0.1183,  0.3097,  0.0119,  0.2327,  0.1350, -0.0287,  0.1196,\n         -0.1189,  0.2064],\n        [-0.0878,  0.0197, -0.3914,  0.3682, -0.4654, -0.0015, -0.0231,  0.2142,\n         -0.2736,  0.2497],\n        [ 0.3123, -0.2719,  0.1713,  0.2054, -0.0769,  0.2460,  0.0503, -0.0538,\n          0.0029,  0.1231],\n        [-0.1932, -0.3040,  0.0577, -0.0959,  0.0891,  0.2135, -0.2945, -0.2178,\n         -0.1516, -0.1464],\n        [-0.1280, -0.2536, -0.3031,  0.2139, -0.3065, -0.2139, -0.0575,  0.2869,\n         -0.2258,  0.2428],\n        [-0.3357,  0.3073, -0.1177, -0.1340, -0.1308, -0.0859,  0.0184,  0.1743,\n          0.1241,  0.2104],\n        [ 0.1361,  0.0650,  0.1923,  0.1875,  0.0581, -0.2783,  0.2209, -0.2247,\n         -0.1489,  0.0398]])\nhidden2.bias tensor([-0.0228,  0.3150, -0.3834, -0.1796, -0.2729, -0.0477,  0.2456, -0.0302])\nhidden3.weight tensor([[-0.2883, -0.0138, -0.2101,  0.0574, -0.2938, -0.1480,  0.3212,  0.0359],\n        [ 0.1073, -0.2964,  0.1617,  0.0427,  0.2994,  0.1690,  0.0290, -0.0803],\n        [ 0.1270, -0.1874,  0.1314,  0.1575,  0.0506,  0.3550, -0.1186,  0.1842],\n        [ 0.1969, -0.0671,  0.3184,  0.1597, -0.0933,  0.2862, -0.1371, -0.3230]])\nhidden3.bias tensor([-0.1524, -0.0063, -0.0454, -0.1961])\noutput.weight tensor([[ 0.5067, -0.3876, -0.4122, -0.2385]])\noutput.bias tensor([-0.3667])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n"
   ]
  },
  {
   "source": [
    "## Print the Misclassification Rate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ True  True  True ...  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "yhat = net(X)\n",
    "yhat = yhat.detach().numpy().reshape(-1) > 0.3\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([False, False, False, ...,  True, False,  True])"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "y = df[:,-1]\n",
    "y = y>5\n",
    "np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5347091932457786\n"
     ]
    }
   ],
   "source": [
    "correctQ = [ i==j for i in yhat for j in y ]\n",
    "print(np.average(correctQ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5347091932457786\n"
     ]
    }
   ],
   "source": []
  },
  {
   "source": [
    "Hmm, misclassificatoin rate is not very good."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}