{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Neural Networks\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "[Tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "source": [
    "## Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Load the Data using pandas/numpy:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./DataSets/winequality-red.csv', sep=';')\n",
    "df=np.array(df.values)"
   ]
  },
  {
   "source": [
    "The last column are the features so extract those:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[:,-1]\n",
    "X = df[:,range(df.shape[1]-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y>5"
   ]
  },
  {
   "source": [
    "Finally make sure that these are tensors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X.astype(np.float32))\n",
    "y = torch.from_numpy(y.astype(np.float32))"
   ]
  },
  {
   "source": [
    "## Torch Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Define a torch class, in this case we'll use an 11-4-1, where the output measures whether or not the wine is good."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Network(\n  (hidden1): Linear(in_features=11, out_features=10, bias=True)\n  (hidden2): Linear(in_features=10, out_features=8, bias=True)\n  (hidden3): Linear(in_features=8, out_features=4, bias=True)\n  (output): Linear(in_features=4, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n  (softmax): Softmax(dim=1)\n)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        # Create the input layer and the hidden layer\n",
    "        self.hidden1 = nn.Linear(11, 10)\n",
    "        self.hidden2 = nn.Linear(10, 8)\n",
    "        self.hidden3 = nn.Linear(8, 4)\n",
    "        self.output = nn.Linear(4, 1)\n",
    "\n",
    "        # Define the activation functions that will be used\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1) # dim=1 calculates softmax across cols\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Take input\n",
    "        x = self.hidden1(x)  # Linear Combination of input-> hidden\n",
    "        x = self.hidden2(x)  # Linear Combination of input-> hidden\n",
    "        x = self.hidden3(x)  # Linear Combination of input-> hidden\n",
    "        x = self.sigmoid(x) # Activation Function\n",
    "        x = self.output(x)  # Linear Combination of hidden -> output\n",
    "        x = self.sigmoid(x) # Activation Function\n",
    "\n",
    "        return x\n",
    "\n",
    "# Assign the model object\n",
    "net = Network()\n",
    "print(net)"
   ]
  },
  {
   "source": [
    "### Generate the Model Output"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.4697],\n        [0.4705],\n        [0.4639],\n        ...,\n        [0.4759],\n        [0.4758],\n        [0.4696]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Print the Model Output\n",
    "out = net(X)\n",
    "print(out)"
   ]
  },
  {
   "source": [
    "## Measure the Loss Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First define a loss function,  We'll use *MSE* here, but there are [many others](https://pytorch.org/docs/stable/nn.html#loss-functions). Also define the form of Gradient Descent implemented"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1/1000\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=eta, momentum = 0.9)"
   ]
  },
  {
   "source": [
    "## Train the Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now that gradient descent model, the structure of the network and the loss function are all defined we can begin training the network."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1,   200] loss: 0.022\n",
      "[1,   400] loss: 0.026\n",
      "[1,   600] loss: 0.024\n",
      "[1,   800] loss: 0.025\n",
      "[1,  1000] loss: 0.025\n",
      "[1,  1200] loss: 0.020\n",
      "[1,  1400] loss: 0.026\n",
      "[2,   200] loss: 0.022\n",
      "[2,   400] loss: 0.024\n",
      "[2,   600] loss: 0.024\n",
      "[2,   800] loss: 0.025\n",
      "[2,  1000] loss: 0.023\n",
      "[2,  1200] loss: 0.020\n",
      "[2,  1400] loss: 0.026\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss=0\n",
    "    for i in range(X.shape[0]):\n",
    "        # Get the input and desired output\n",
    "        input   = X[i,:]\n",
    "        target  = y[i]\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward, then backward, then optimize\n",
    "        ## Calculate output\n",
    "        outputs = net(input)\n",
    "        ## Measure the Loss\n",
    "        loss   = criterion(outputs, target)\n",
    "        ## Calculate the Gradients\n",
    "        loss.backward()\n",
    "        ## Adjust the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        ## Print any Statistics\n",
    "        running_loss += loss.item()\n",
    "        # print(loss.item())\n",
    "        if i % 200 == 199:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "   \n",
    "print('Finished Training')\n"
   ]
  },
  {
   "source": [
    "### Save the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './wine_neural_network.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "source": [
    "## Print the Parameters of the Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hidden1.weight tensor([[-2.9496e-01, -1.8367e-01,  1.7613e-01,  7.1353e-04, -2.0044e-01,\n         -2.2062e-01, -1.3609e-01, -1.1500e-01,  1.5059e-01,  1.4561e-01,\n         -3.4647e-01],\n        [-1.1676e-01, -5.9055e-02,  2.0699e-01,  1.7329e-01, -1.0591e-01,\n         -4.9562e-02, -2.7079e-01, -6.5656e-02,  2.9439e-01,  1.0861e-01,\n          2.3687e-01],\n        [ 2.1225e-01,  1.7519e-01,  1.7001e-01, -1.3865e-04, -1.8732e-01,\n         -2.1608e-01, -2.2757e-02, -7.4580e-02,  2.6850e-01,  2.5534e-01,\n          9.3617e-02],\n        [-2.4720e-01, -2.6311e-02, -2.0202e-01,  2.3485e-02,  2.1087e-01,\n         -3.3093e-02, -5.6192e-02,  1.7933e-01, -2.1967e-01,  1.6400e-01,\n         -2.1926e-01],\n        [ 1.8310e-01, -2.8201e-01,  3.1032e-02,  2.9673e-01,  2.2340e-01,\n         -2.7925e-02, -3.1455e-01, -1.9614e-01, -9.2661e-02,  1.8298e-01,\n         -2.4908e-02],\n        [-7.0997e-02, -8.3039e-02,  1.1082e-01,  2.0215e-01,  2.3935e-01,\n          2.4391e-01,  2.4319e-01,  2.1143e-01, -3.2762e-02,  1.8142e-01,\n          2.2800e-01],\n        [-2.6315e-01,  2.3609e-01,  1.3479e-01,  5.9020e-02,  1.5604e-01,\n         -2.9061e-01,  4.0076e-02,  1.2092e-01,  1.4990e-01, -2.1303e-01,\n          9.5246e-02],\n        [-1.1631e-02, -2.3227e-01, -2.8332e-01, -2.6579e-01,  5.9710e-02,\n          3.3556e-01, -2.6742e-01,  4.5888e-02,  7.8068e-02,  2.6348e-01,\n          1.4902e-01],\n        [-1.9023e-01,  1.3532e-01, -7.2328e-02,  5.9105e-02, -1.2521e-02,\n         -7.4514e-02, -1.5307e-01, -7.5064e-03, -1.5155e-02, -4.3673e-02,\n          1.4748e-01],\n        [-2.1650e-01,  1.5083e-01, -1.4684e-01,  2.1189e-01, -1.3244e-01,\n         -1.7263e-01,  6.0474e-02,  8.6752e-02, -2.2886e-01,  2.2330e-01,\n          2.5305e-01]])\nhidden1.bias tensor([-0.0614,  0.1869,  0.1752, -0.0319, -0.1646, -0.0724,  0.0101, -0.0500,\n        -0.0117, -0.0742])\nhidden2.weight tensor([[ 1.4317e-01, -1.1820e-01,  2.6578e-01, -1.1789e-01,  4.0880e-02,\n         -2.2855e-02,  2.0033e-01, -2.9096e-01,  1.8042e-01, -1.7963e-01],\n        [ 2.8104e-01,  1.6443e-01,  2.4813e-01, -4.9793e-02, -2.6477e-01,\n         -1.2730e-01, -6.7986e-02, -3.3166e-01,  2.2173e-01,  1.5441e-01],\n        [-1.1989e-01,  9.9471e-02, -9.5709e-02,  8.2301e-04, -1.6344e-01,\n          1.5078e-01, -1.6589e-01,  3.0689e-01,  1.7058e-01, -2.7119e-02],\n        [ 1.9307e-01, -1.1145e-02, -2.3484e-01, -2.0504e-01, -1.1226e-01,\n         -2.4205e-01,  9.2314e-02, -5.6497e-02, -1.2023e-01, -1.0223e-01],\n        [-3.4414e-02, -2.6015e-01,  2.0367e-01,  5.7033e-02,  1.5197e-01,\n          2.2071e-01,  2.3781e-01,  1.8515e-01, -2.7489e-01,  2.9469e-01],\n        [ 5.4872e-02, -1.1765e-02, -2.7386e-02, -1.6755e-01,  1.8226e-01,\n          2.5653e-01, -1.3582e-01,  1.4777e-01, -2.1223e-04, -7.6386e-02],\n        [-1.9238e-01,  1.5235e-01, -8.4722e-02, -2.9619e-01,  2.8925e-01,\n         -1.1792e-01, -6.6252e-02,  2.0689e-01,  1.1440e-01,  1.8736e-01],\n        [-2.9929e-02,  2.9754e-01,  2.4552e-01, -1.3690e-01, -9.2344e-02,\n         -2.0416e-01,  1.2006e-01, -1.9017e-02, -1.4869e-01, -5.0454e-02]])\nhidden2.bias tensor([-0.1183, -0.0056,  0.0940, -0.2398,  0.2932, -0.2625, -0.0770, -0.0274])\nhidden3.weight tensor([[ 0.2277,  0.1558,  0.1495,  0.1545,  0.1506,  0.2145,  0.3176,  0.1891],\n        [-0.2173, -0.1598, -0.0781, -0.3708, -0.0396,  0.0205,  0.2406, -0.0616],\n        [ 0.1722,  0.1369, -0.0218,  0.2329, -0.0876, -0.2831,  0.2234,  0.3455],\n        [ 0.0794,  0.2308, -0.2541, -0.1292,  0.0411, -0.0381, -0.3550,  0.2154]])\nhidden3.bias tensor([ 0.1521,  0.1167, -0.2731, -0.2264])\noutput.weight tensor([[ 0.2128,  0.5107,  0.3616, -0.3469]])\noutput.bias tensor([-0.2013])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n"
   ]
  },
  {
   "source": [
    "## Print the Misclassification Rate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ True  True  True ...  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "yhat = net(X)\n",
    "yhat = yhat.detach().numpy().reshape(-1) > 0.3\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([False, False, False, ...,  True, False,  True])"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "y = df[:,-1]\n",
    "y = y>5\n",
    "np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctQ = [ i==j for i in yhat for j in y ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5347091932457786\n"
     ]
    }
   ],
   "source": [
    "print(np.average(correctQ))"
   ]
  },
  {
   "source": [
    "Hmm, so even though the loss is quite low, it seems that in misclassificatoin rate is not very good."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}