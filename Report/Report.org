#+TITLE: Implementing of RankNet
:PREAMBLE:
#+OPTIONS: broken-links:auto todo:nil H:9 tags:t tex:t
#+STARTUP: overview
#+AUTHOR: Ryan Greenup
#+PLOT: title:"Citas" ind:1 deps:(3) type:2d with:histograms set:"yrange [0:]"
# #+TODO: TODO IN-PROGRESS WAITING DONE
#+CATEGORY: TAD
:END:
:HTML:
#+INFOJS_OPT: view:info toc:3
#+HTML_HEAD_EXTRA: <link rel="stylesheet" type="text/css" href="./resources/style.css">
# #+CSL_STYLE: /home/ryan/Templates/CSL/nature.csl
:END:
:R:
#+PROPERTY: header-args:R :session TADMain :dir ./ :cache yes :eval never-export :exports both
#+PROPERTY: :eval never
# exports: both (or code or whatever)
# results: table (or output or whatever)
:END:
:LATEX:
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper,11pt,twoside]
#+LATEX_HEADER: \IfFileExists{./resources/style.sty}{\usepackage{./resources/style}}{}
#+LATEX_HEADER: \IfFileExists{./resources/referencing.sty}{\usepackage{./resources/referencing}}{}
#+LATEX_HEADER: \addbibresource{../resources/references.bib}
#+LATEX_HEADER: \usepackage[mode=buildnew]{standalone}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usetikzlibrary{decorations.fractals}
#+LATEX_HEADER: \usetikzlibrary{lindenmayersystems}
:END:
@@latex: \newpage @@

* Introduction
  Ranknet is an approach to /Machine-Learned Ranking (often refered to
  as "//Learning to Rank//" cite:liuLearningRankInformation2009) that
  began development at Microsoft from 2004 onwards
  cite:christopherburgesRankNetRankingRetrospective2015, although
  previous work in this area had already been undertaken as early as
  the 90s (see generally
  cite:fuhrProbabilisticModelsInformation1992,fuhrOptimumPolynomialRetrieval1989,fuhrProbabilisticModelsInformation1992,geyInferringProbabilityRelevance1994,wongLinearStructureInformation1988)
  these earlier models didn't perform well compared to more modern
  machine learning techniques
  [[cite:manningIntroductionInformationRetrieval2008][\S 15.5]].

  Information retrieval is an area that demands effective ranking of
  queries, although straight-forward tools such as =grep=, relational
  databases (e.g. sqlite, MariaDB, PostgreSql) or NoSQL (e.g. CouchDB,
  MongoDB) can be used to retrieve documents with matching characters
  and words, these methods do not perform well in real word tasks
  across large collections of documents because they do not provide
  any logic to rank results (see generally
  cite:viksinghComparisonOpenSource2009).


* Motivation

  Search Engines implement more sophisticated techniques to rank
  results, one such example being TF-IDF weighting
  cite:martyschochBleveSearchDocumentation , well established
  search engines such as /Apache Lucene/
  cite:apachesoftwarefoundationLearningRankApache2017 and /Xapian/
  cite:jamesaylettGSoCProjectIdeasLearningtoRankStabilisationXapian2019,
  however, 
  are implementing Machine-Learned Ranking in order to improve results.

  This paper hopes to serve as a general introduction to the implementation
  of the Ranknet technique to facilitate developers of search engines in
  more modern languages (i.e. =Go= and =Rust=) in implementing
  it. This is important because these more modern languages are more
  accessible cite:huntThesisSubmittedPartial
  and memory safe cite:perkelWhyScientistsAre2020 than C/C++
  respectfully, without significantly impeding performance; this will
  encourage contributors from more diverse backgrounds and hence
  improve the quality of profession-specific tooling.

  
  For a non-comprehensive list of actively maintained search engines,
  see \S [[#search-engines-list]] of the appendix.

* Implementation

  The Ranknet approach is typically implemented using Neural Networks,
  an early goal of this research was to evaluate the performance of
  different machine learning algorithms to implement the Ranknet
  method, this research however is still ongoing.

  Further Research to look at the implementation of Ranknet for
  documents and comparing different approaches to apply the method to
  on-demand queries is required, although this seems to have been
  implemented by open source Apache Solr project
  cite:michaelalcornIntroductionMachinelearnedRanking, which may
  provide guidance for further study. teamDocFetcherFastDocument.

  
  Ranking/ is the process of applying machine learning algorithms to
  ranking problems, it .

  A lot of data cannot be clearly categorised or quantified even if there
  is a capacity to compare different samples, the motivating example
  is a collection of documents, it might be immediately clear to the
  reader which documents are more relevant than others, even if the
  reader would not be able to quantify a "relevance score" for each
  document.

  By training a model to identify a more relevant document, a ranking
  can be applied to the data.

  An example of this might be identifying documents in a companies
  interwiki that are relevant for new employees, by training the model
  to rank whether one document is more relevant than an other,
  ultimately an ordered list of documents most relevant for new
  employees could be created.
* Implementation
  This implementation will first apply the approach to a simple data
  set so as to clearly demonstrate that the approach works, following
  that the model will be extended to support wider and more complex
  data types before finally being implemented on a corpus of documents.

** Neural Network
   Neural Networks cite:pictonNeuralNetworks1994

   The Ranknet method is typically implemented using a Neural Network,
   although other machine learning techniques can also be used
   [[cite:christopherburgesRankNetRankingRetrospective2015][\s 1]],
   Neural Networks are essentially a collection of different
   regression models that are fed into one another to create a
   non-linear classifier, a loss function is used to measure the
   performance of the model with respect to the parameters
   (e.g. RMSE [fn:1] or BCE [fn:2]) and the parameters are adjusted so
   as to reduce this error by using the /Gradient Descent Technique/
   (although there are other optimisation algorithms such as RMSProp
   and AdaGrad cite:mukkamalaVariantsRMSPropAdagrad2017 that can be
   shown to perform better see
   cite:bushaevUnderstandingRMSpropFaster2018). The specifics of
   Neural Networks are beyond the scope of this paper (see
   cite:hmkcodeBackpropagationStepStep or more generally cite:pictonNeuralNetworks1994).

*** The Ranknet Method

   The Ranknet method is concerned with a value \(p_{ij}\) that
   measures the probability that an observation \(i\) is ranked higher
   than an observation \(j\).

   A Neural Network (\(n\)) is trained to return a value
   \(s_k\) from a feature vector \(\mathbf{X}_k\):

   \[n(\mathbf{X}_i) = s_i \quad \exists k\]
  So as to minimise the error of:


  \[
  p_{ij} = \frac{1}{1+e^{\sigma \cdot (s_i-s_j)}} \quad \exists \sigma
  \in \mathbb{R}
  \]
  
*** Implementation
    The first step is to create a simple data set and design a neural
    network that can classify that data set, this can then be extended.
    
** TODO How to clone 
   How can the reader clone this onto there machine?

   put on the summer repo then provide instructions to clone this
   working example onto there machine to try it out.
** TODO Blobs
** TODO Moons
** TODO Optimisers
** TODO Batches
** TODO Wine
** TODO Rank Wiki Articles
* TODO Difficulties
  - Don't use torch
    - Do it by hand first because it can be hard to see if the correct
      weights are being updated sensibly, making debugging very difficult.
    - R or Julia would be easier because counting from 0 get's pretty
      confusing when dealing with {1, 0}, {-1, 0, 1}.
  - Don't use misclassification rate to measure whether the ranking
    - In hindsight this is obvious, but at the time misclassification
      was a tempting metric because of it's interpretability
    was correct

    Very difficult to see if the model is working

  - A continuous function will still produce an ordered pattern in
      the ranking of results, even if the model hasn't been trained,
      so visualising isn't helpful either.

  - Implement it on a data set that already has order, obfuscate the
      order and then contrast the results
    - or use a measurement

  - Plot the loss function of the training data live, the model is
    slow to train and waiting for it to develop was a massive time
    drain.
    


* Further Research

  It is still not clear how the
  performance of Ranknet compares to traditional approaches
  implemented by search engines (see \S [[#search-engines-list]]), further
  study would ideally:

  - Write a program to query a corpus of documents using an existing search engine.
    - Or possibly just implement TF-IDF weighting in order to remove variables.
  - Extend the program to implement machine learned ranking
  - Measure and contrast the performance of the two models to see
    whether there are any significant improvements.

  This could be implemented with TREC datasets
  cite:usnationalinstituteofstandardsandtechnologyTextREtrievalConference
  using a cummulated-gain cost function
  cite:jarvelinCumulatedGainbasedEvaluation2002 as demonstrated in
  previous work cite:viksinghComparisonOpenSource2009.

* Conclusion

* Further Research

  - Apply this to documents to get a sorted list.
  - The "Quicksort" algorithm likely needs a random pivot to be efficient cite:timroughgardenQuicksortOverview2017

* Text and References
Fractals are complex shapes that often occur from natural processes, in this
report we hope to investigate the emergence of patterns and complex structures
from natural phenomena. We begin with an investigation into fractals and the
concept of dimension and then discuss links between fractal patterns and natural
processes.

This is a Reference cite:tuGraphBasedSemiSupervisedNearestNeighbor2016a and another cite:nicodemiIntroductionAbstractAlgebra2007a and yet another cite:christopherburgesRankNetLambdaRankLambdaMART2010.

* Fractals
Images are shown in figure [[imtest]].

# #+NAME: imtest
# #+CAPTION: This is a test image showing the outline of a Julia set
# #+attr_html: :width 400px
# #+attr_latex: :width 0.5\textwidth
[[# file:media/outline-rabbit.png]]

* Appendix
  
** Search Engines
   :PROPERTIES:
   :CUSTOM_ID: search-engines-list
   :END:
There are many open source search engines available , a cursory review
found the following popular projects:

- [[https://github.com/cyclaero/zettair][Zettair]] (=C=) cite:jansenCyclaeroZettair2020
- [[https://github.com/apache/lucene-solr][Apache lucene/Solr]] (=Java=) cite:apachesoftwarefoundationLearningRankApache2017
  - Implemented by [[https://sourceforge.net/p/docfetcher/code/ci/master/tree/][DocFetcher]] cite:docfetcherdevelopmentteamDocFetcherFastDocument
- [[https://github.com/sphinxsearch/sphinx][Sphinx]] (=C++=) cite:yurischapovSphinxsearchSphinx2021
- [[https://github.com/kevinduraj/xapian-search][Xapian]] (=C++=) cite:ollybettsXapianXapian2021
  - Implemented by [[https://www.lesbonscomptes.com/recoll/][Recoll]] cite:jean-francoisdockesRecollUserManual

More Modern Search engines include:

- [[https://github.com/olivernn/lunr.js/][LunrJS]]  (=JS=) cite:nightingaleOlivernnLunrJs2021
- [[https://github.com/blevesearch/bleve][Bleve Search]] (=Go=) cite:martyschochBleveSearchDocumentation
- [[https://github.com/go-ego/riot][Riot]] (=Go=) cite:vzGoegoRiot2021
- [[https://github.com/tantivy-search/tantivy][Tantivy]] (=Rust=) cite:clementrenaultMeilisearchMeiliSearch2021
- [[https://github.com/andylokandy/simsearch-rs][SimSearch]] (=Rust=) cite:lokAndylokandySimsearchrs2021

  
*** Fuzzy String Match
    Somewhat related are programs that rank string similarity, such programs don't tend
    to perform well on documents however (so for example these would
    be effective to filter document titles but would not be useful for
    querying documents):

    - [[https://github.com/junegunn/fzf][=fzf=]] cite:choiJunegunnFzf2021
    - [[https://github.com/jhawthorn/fzy][=fzy=]] cite:hawthornJhawthornFzy2021
    - [[https://github.com/peco/peco][=peco=]] cite:lestrratPecoPeco2021
    - [[https://github.com/lotabout/skim][Skim]] cite:zhangLotaboutSkim2021
    - [[https://github.com/lotabout/skim][=go-fuzzyfinder=]] cite:ktrKtr0731Gofuzzyfinder2021
    - [[https://github.com/lotabout/skim][Swiper]] cite:krehelAboaboSwiper2021

* Footnotes

[fn:2] *BCE* /Binary Cross Entropy/ 

[fn:1] *RMSE* /Root Mean Square Error/  
